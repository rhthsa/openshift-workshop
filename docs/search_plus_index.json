{"./":{"url":"./","title":"Introduction","keywords":"","body":"OpenShift Cloud-Native Application Developer Advocacy Workshop Gitbook Gitbook here Main Labs Deploy application to openshift with Source-to-Image Basic Openshift Topology Scaling up your application in order to handle workload OOTB Application Monitoring, Alert & User workload monitoring Complex Cloud-Native Application with Live Flight Track Demo Optional labs App Configuration App Health check Viewing and working with logs generated by your application Basic Container Storage Serverless "},"deploywiths2i.html":{"url":"deploywiths2i.html","title":"Deploy","keywords":"","body":"Deploy application (quarkus) to openshift with s2i Deploy application (quarkus) to openshift with s2i Openshift Project Deploy Application to Openshift with OpenShift Developer Console (S2I) Test Rest API of Backend Application Next Step Openshift Project open browser to https://console-openshift-console.apps.xxx.opentlc.com confirm URL from instructor login to openshift with your username/password username: 'userx' password: openshift select Developer Perspective from left menu (if openshift don't default page for you) default project is your name. such as 'user1', 'user2' if login first time, dev console will launch developer quick tour, click Skip tour if deveveloper console not default your project, click Project name 'PR userx' in topology or select from Project Dropdownlist Optional: How to Create Project (Don't create project in this workshop, we prepare project for you now!) create project with your username such as 'user1' go to dropdown at Project: All Projects click Create Project set Name*= 'userx', Display Name = 'userx', Description = 'userx workshop' (Please use your username to Project name!!!) click create, openshift console will change page to new project Deploy Application to Openshift with OpenShift Developer Console (S2I) click +Add menu in left pane select Import from Git in Import from Git page, input Git Repo URL with 'https://github.com/chatapazar/openshift-workshop.git' wait until Openshift validate URL complete (page will show validated complete icon) Optional: Not required for this lab! you can input additional information for get source code such as Git Reference: for branch, tag, or commit. (default s2i will checkout from default branch such as main or master) Context dir: in case source code don't place in root of git such as /code Source Secret: provide user/password for private repository OpenShift S2I will automatic select Builder Image from your source code, in case s2i can't detect base image. you can manual select. developer can select builder image version from dropdown list such as java application can select base image for jdk8 or jdk11 click 'Edit Import Strategy' for this workshop, Please select 'openjdk-11-ubi8' or Red Hat OpenJDK 11 (UBI 8) (default is openjdk-17-ubi8, it's new jdk version. it don't match with simple code in this demo! :D ) next, in general section set Application name: backend Name: backend Resources: select Deployment (deployment for standard Kubernetes, DeploymentConfig is deployment with extension feature from OpenShift) Advanced Options: checked Create a Route to the Application before click create, in advanced option click 'Labels' link add label 'app=backend' click 'Resource limits' link set CPU Request: 100 millicores set CPU Limit: 200 millicores set Memory Request: 256 Mi set Memory Limit: 512 Mi Click Create, Console will back to Topology Page Click at Duke icon, Dev Console will show Deployment information in build section, OpenShift Build is creating image with S2I click View logs at Build #1 wait until build complete, see build backend-1 change from running to complete (3-5 minutes) after build complete, openshift will take the image from the build to deploy and create a pod as shown in the picture. wait until Pod 'backend-xxxx-xxx' change status to Running click View logs, check application start complete (wait until logging show message 'Installed features: ....') click Topology in left pane, at duke icon, you will see Open URL link, click it to open browser to this application in new tab Or Click Location from Routes 'backend' in 'backend' Deployment information Example of backend application landing page Deployment Done! Test Rest API of Backend Application click '>_' icon in top of Openshift Web Console to open Web Terminal First Time, Web Terminal will ask you for project to initialze terminal, please select your project such as user1, click start wait until terminal start complete, command prompt will show in command line terminal Next time, you can press from the icon '>_' immediately without creating a new one. in command line terminal, check current project by below command oc project example of output Using project \"user1\" from context named \"user1-context\" on server \"https://172.30.0.1:443\". if current project is not your project (such as result is not 'Using project \"user1\"'), use below command to set current project to command line context, change \"user1\" in command to your project name. oc project user1 test call backend service api (REST)curl https://$(oc get route backend -o jsonpath='{.spec.host}')/backend example of resultBackend version:v1, Response:200, Host:backend-7b5c56fc8c-t57wl, Status:200, Message: Hello, World remark: Host name in result of this api is name of Pod, please check and verify it! if done!, You are ready for the next step.Next Step Basic Openshift Topology "},"openshifttopology.html":{"url":"openshifttopology.html","title":"Basic Openshift Topology","keywords":"","body":"Basic Openshift Topology Basic Openshift Topology Prerequisite Pods Labels Deployment Service Route Next Step Prerequisite Complete Deploy java application (quarkus) to openshift with s2i Go to your project (same as your username) Open Web Terminal by click '>_' on top of OpenShift Web Console use web terminal to run command line Pods A pod is a collection of containers sharing a network, acting as the basic unit of deployment in Openshift. All containers in a pod are scheduled on the same node.To launch a pod using the container imagequay.io/openshiftlabs/simpleservice:0.5.0 and exposing a HTTP API on port 9876, execute: oc run sise --image=quay.io/openshiftlabs/simpleservice:0.5.0 --port=9876 wait until sise pod circle change to dark blue Check to see if the pod is running: oc get pods Resulting in output similar to the following: NAME READY STATUS RESTARTS AGE sise 1/1 Running 0 2m This container image happens to include a copy of curl, which provides an additional way to verify that the primary webservice process is responding (over the local net at least): oc exec sise -t -- curl -s localhost:9876/info This call should produce the output: {\"host\": \"localhost:9876\", \"version\": \"0.5.0\", \"from\": \"127.0.0.1\"} or use Openshift Web Console to terminal to this Pod In Topology Menu, click Pod 'sise' In Pod sise information (right side), click pods 'sise' In Pod detail page, click tab 'Terminal' Openshift will create terminal to your container in Pod, see below picture test call curl command in terminalcurl -s localhost:9876/info click Topology menu in left pane to back to topology of your project clean up your 'sise' pod back to web terminal in buttom frame at command prompt call below command for clean up pod (click restart terminal if your terminal is terminated)oc delete pod sise wait until pod 'sise' remove from your topology Labels Labels are the mechanism used to organize Kubernetes objects. A label is a key-value pair with certain restrictions concerning length and allowed values but without any pre-defined meaning. You're free to choose labels as you see fit, for example, to express environments such as \"this pod is running in production\" or ownership, like \"department X owns that pod\". Let's create a pod that initially has one label (env=development): Open Your Web Terminal and Check Current Project (same username) run yaml for create pod from https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/labels/pod.yamloc apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/labels/pod.yaml The get subcommand can be used to display a pod's labels:oc get pods --show-labels The labels are rendered as an additional column in the output:NAME READY STATUS RESTARTS AGE LABELS labelex 1/1 Running 0 6s env=development edit label with Openshift Web Console click Pod 'labelex' on Topology page in Pod 'labelex' information click tab Details click edit link (near pencil icon) at lables section to view Edit labels add 'owner=userx', change userx to your username such as user1 and save view change use label for select Openshift Object To use a label for filtering, use the -l option: (change user1 to your username)oc get pods -l owner=user1 clean up pod 'labelex'oc delete pods labelex Deployment A deployment is a supervisor for pods, giving you fine-grained control over how and when a new pod version is rolled out as well as rolled back to a previous state. Let's create a deployment called sise-deploy that produces two replicas of a pod as well as a replica set: (from https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/deployments/d09.yaml)oc apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/deployments/d09.yaml wait to deployment done, check result from Topology Page check result from Web Terminal oc get pod,replicaset,deployment The result is separated by resource type and reflects all of the resources created by the deployment: NAME READY STATUS RESTARTS AGE pod/sise-deploy-747848cd97-j2m9q 1/1 Running 0 90s pod/sise-deploy-747848cd97-kr9kd 1/1 Running 0 90s NAME DESIRED CURRENT READY AGE replicaset.apps/sise-deploy-747848cd97 2 2 2 90s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/sise-deploy 2/2 2 2 90s At this point in time the sise containers running in the pods are configured to return the version 0.9. Let's verify this from within the cluster using curl:POD=$(oc get pods --no-headers -l app=sise | grep sise |head -n 1| awk '{print $1}') oc exec $POD -t -- curl -s 127.0.0.1:9876/info The output reflects the current version of the deployed application:{\"host\": \"127.0.0.1:9876\", \"version\": \"0.9\", \"from\": \"127.0.0.1\"} Let's now see what happens if we change that version to 1.0 in an updated deployment: https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/deployments/d10.yamloc apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/deployments/d10.yaml wait until redeploy complete!, in Topology , you will can see deployment redeploy from 0.9 to 1.0 run command to check have redeploy to new versionoc get pods -l app=sise result of commandNAME READY STATUS RESTARTS AGE sise-deploy-67fd84bd5c-cvwpk 1/1 Running 0 35s sise-deploy-67fd84bd5c-wn9zh 1/1 Running 0 38s To verify that if the new 1.0 version is really availablePOD=$(oc get pods --no-headers -l app=sise | grep sise |head -n 1| awk '{print $1}') oc exec $POD -t -- curl -s 127.0.0.1:9876/info The output reflects the current version of the deployed application:{\"host\": \"127.0.0.1:9876\", \"version\": \"1.0\", \"from\": \"127.0.0.1\"} A history of all deployments is available via the rollout history subcommand:oc rollout history deploy/sise-deploy resultdeployment.apps/sise-deploy REVISION CHANGE-CAUSE 1 2 to rollback to the last successful deployed revision of your configurationoc rollout undo deploy/sise-deploy wait until reploy complete (sise-deploy circle change to dark blue color), and retest call with curl command Clean Upoc delete deploy sise-deploy Service Services A service is an abstraction for pods, providing a stable, so called virtual IP (VIP) address. While pods may come and go and with it their IP addresses, a service allows clients to reliably connect to the containers running in the pod using the VIP. The \"virtual\" in VIP means it is not an actual IP address connected to a network interface, but its purpose is purely to forward traffic to one or more pods. Keeping the mapping between the VIP and the pods up-to-date is the job of kube-proxy, a process that runs on every node, which queries the API server to learn about new services in the cluster.Let's create a new pod supervised by a replication controller and a service along with it: run below command in web terminal : 1st commond for deployment object and 2nd command for service https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/manifest/deployment.yaml https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/manifest/svc.yamloc apply -f https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/manifest/deployment.yaml oc apply -f https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/manifest/svc.yaml see deployment, pod and service in topology view Verify the pod is running:oc get pods -l app=sise get Pod NamePOD=$(oc get pods --no-headers -l app=sise | grep sise |head -n 1| awk '{print $1}') echo $POD A new pod name should be generated each time this example is run. Make sure to include your own pod name when running the following examples:oc describe pod $POD The output should appear similar to the following (which has been truncated for readability):... Status: Running IP: 10.131.0.51 IPs: IP: 10.131.0.51 Controlled By: ReplicaSet/sise-558d96fc8c ... You can, from within the cluster, access the pod directly via its assigned IP (change pod name and ip address from describe pod):oc exec $POD -t -- curl -s :9876/info This is however, as mentioned above, not advisable since the IPs assigned to pods may change as pods are migrated or rescheduled. The service created at the start of this lesson, simpleservice, is used to abstract the access to the pod away from a specific IP:oc get service/simpleservice From within the cluster, we can now access any affiliated pods using the IP address of the simpleservice svc endpoint on port 80. KubeDNS even provides basic name resolution for Kubernetes services (within the same Kubernetes namespace). This allows us to connect to pods using the associated service name - no need to including IP addresses or port numbers.oc exec $POD -t -- curl -s simpleservice/info Let’s now add a second pod by scaling up the RC supervising it:oc scale --replicas=2 deployment/sise Wait for both pods to report they are in the \"Running\" state:oc get pods -l app=sise sample outputNAME READY STATUS RESTARTS AGE sise-558d96fc8c-7nblw 1/1 Running 0 18s sise-558d96fc8c-r7wsw 1/1 Running 0 13m see pod in deployment info page test call service, you can see out put from 2 podsoc exec $POD -t -- curl -s simpleservice/info sample output oc exec $POD -t -- curl -s simpleservice/info {\"host\": \"simpleservice\", \"version\": \"0.9\", \"from\": \"10.131.0.38\"} oc exec $POD -t -- curl -s simpleservice/info {\"host\": \"simpleservice\", \"version\": \"0.9\", \"from\": \"10.131.0.1\"} oc exec $POD -t -- curl -s simpleservice/info {\"host\": \"simpleservice\", \"version\": \"0.9\", \"from\": \"10.131.0.38\"} oc exec $POD -t -- curl -s simpleservice/info {\"host\": \"simpleservice\", \"version\": \"0.9\", \"from\": \"10.131.0.1\"} Route route exposes a service at a host name, like www.example.com, so that external clients can reach it by name. create route for serviceoc expose service/simpleservice get route from command lineoc get route simpleservice example outputNAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD simpleservice simpleservice-user1.apps.cluster-4b8f.4b8f.sandbox930.opentlc.com simpleservice 9876 None or get route from web console, go to topology, click sise deployment, view Route test call route from command line (in your laptop or web terminal)curl http://$(oc get route simpleservice -o jsonpath='{.spec.host}')/info example bash-4.4 ~ $ oc get route simpleservice NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD simpleservice simpleservice-user1.apps.cluster-4b8f.4b8f.sandbox930.opentlc.com simpleservice 9876 None bash-4.4 ~ $ curl http://simpleservice-user1.apps.cluster-4b8f.4b8f.sandbox930.opentlc.com/info {\"host\": \"simpleservice-user1.apps.cluster-4b8f.4b8f.sandbox930.opentlc.com\", \"version\": \"0.9\", \"from\": \"10.128.2.26\"} bash-4.4 ~ $ clean up application oc delete route simpleservice oc delete -f https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/manifest/svc.yaml oc delete -f https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/manifest/deployment.yaml Next Step Scaling up your application in order to handle workload "},"scale.html":{"url":"scale.html","title":"Scaling","keywords":"","body":"Scaling up your application in order to handle workload Scaling up your application in order to handle workload Prerequisite Manual Scale Application Auto Scale Application Next Step Prerequisite Complete Deploy application to openshift with s2i Go to your project (same as your username) Open Web Terminal by click '>_' on top of OpenShift Web Console use web terminal to run command line Manual Scale Application click topology in left menu, click Duke icon (backend deployment), Details tab click increase ther pod count (^ icon) to 2 Pod wait until application scale to 2 Pods (circle around Duke icon change to dark blue) Test load to application, go to web terminal, run below command BACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') while [ 1 ]; do curl $BACKEND_URL/backend printf \"\\n\" sleep 10 done example result, check have result from 2 pods (Host value)Backend version:v1, Response:200, Host:backend-95647fbb8-kt886, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-95647fbb8-q9dqv, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-95647fbb8-kt886, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-95647fbb8-q9dqv, Status:200, Message: Hello, World after few minute, type 'ctrl-c' in web terminal to terminated curl command go to Resources Tab, in Pods section, show 2 pods after scale click 'View logs' of 1st Pod and 2nd Pod to confirm both pod are processed. example of 1st pod example of 2nd pod back to detail pages of backend deployment, scale pod to 0 (for this case, no pod for this application) scale backend to 1 pod Auto Scale Application add HorizontalPodAutoscaler go to Topology, click at Duke icon for open backend deployment, click action dropdown menu, select Add HorizontalPodAutoscaler in Add HorizontalPodAutoscaler, use Form view set Name: example Minimum Pods: 1 Maximum Pods: 3 CPU Utilization: 1% click save, and wait until backend deployment change to Autoscaling load test to backend application for proof auto scale go to web terminal run load test commandBACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') while [ 1 ]; do curl $BACKEND_URL/backend printf \"\\n\" done click detail tab of backend deployment, wait until autoscaled to 3 (wait a few minutes) click resources tab, see 3 pods auto scale click Observe tab to view CPU usage back to web terminal, input 'ctrl-c' to terminate load test command wait 5 minute, autoscaled will reduce pod to 1. remove HorizontalPodAutoscaler, go to backend deployment information page, select action menu, select remove HorizontalPodAutoscaler confirm Remove, and wait until backend change to manual scale Optional: if you don't want to wait autoscale down to 1 pod, you can remove HorizontalPodAutoscaler and manual scale down to 1 by yourself. Next Step OOTB Application Monitoring & User workload monitoring "},"monitor.html":{"url":"monitor.html","title":"Application Monitoring","keywords":"","body":"OOTB Application Monitoring, Alert & User workload monitoring OOTB Application Monitoring, Alert & User workload monitoring Prerequisite OpenShift Default Monitoring Review Application Performance Metric Code Add Application Performance Metric to OpenShift Add Alert to OpenShift Next Step Prerequisite Complete Deploy application to openshift with s2i Go to your project (same as your username) Open Web Terminal by click '>_' on top of OpenShift Web Console use web terminal to run command line OpenShift Default Monitoring view defalt monitoring per deployment, click topology, click duke icon (backend deployment), in backend deployment, select observe tab view CPU usage, view Memory usage view Project Monitoring, click Observe in left menu, select Dashboard Tab this page will show Monitoring Information of current project (all resources in this project) can filter by workload, click at dashboard dropdownlist and select Kubernetes/Compute Resources/Workload, type deployment, workload backend select tab Metrics to view performance/metrics information by type, click select query dropdown list to select default metrics information such as cpu usage, memory usage, filesystem usage, etc. select CPU usage, click check box 'Stacked' change to another metrics such as memory usage. OpenShift Monitoring base on Prometheus Technology, you can use PromQL for retrive metric information, in select query dropdown list, select Custom query and type 'cpu' and wail auto suggestion, select 'pod:container_cpu_usage:sum' and type 'enter' button to view this metrics from PromQL click Alerts tab to view all alert (the Alerting UI enables you to manage alerts, silences, and alerting rules, we will create alert in next step in this session) click Events Tab to view All event in this project or filter by resource Review Application Performance Metric Code Developer can enable monitoring for user-defined projects in addition to the default platform monitoring. You can now monitor your own projects in OpenShift Container Platform without the need for an additional monitoring solution. review application metric code backend application use quarkus microprofile metrics libraly to generate application metrics example code: https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/src/main/java/org/acme/getting/started/BackendResource.java example custom metrics in code:@Counted( name = \"countBackend\", description = \"Counts how many times the backend method has been invoked\" ) @Timed( name = \"timeBackend\", description = \"Times how long it takes to invoke the backend method in second\", unit = MetricUnits.SECONDS ) @ConcurrentGauge( name = \"concurrentBackend\", description = \"Concurrent connection\" ) public Response callBackend(@Context HttpHeaders headers) throws IOException { review example metrics of backend application, go to web terminal call default quarkus microprofile metrics exampleoc exec $(oc get pods -l app=backend | grep backend | head -n 1 | awk '{print $1}') \\ -- curl -s http://localhost:8080/q/metrics example result... # HELP vendor_memoryPool_usage_max_bytes Peak usage of the memory pool denoted by the 'name' tag # TYPE vendor_memoryPool_usage_max_bytes gauge vendor_memoryPool_usage_max_bytes{name=\"CodeHeap 'non-nmethods'\"} 1352064.0 vendor_memoryPool_usage_max_bytes{name=\"CodeHeap 'non-profiled nmethods'\"} 1018240.0 vendor_memoryPool_usage_max_bytes{name=\"CodeHeap 'profiled nmethods'\"} 5218944.0 vendor_memoryPool_usage_max_bytes{name=\"Compressed Class Space\"} 3856880.0 vendor_memoryPool_usage_max_bytes{name=\"Metaspace\"} 3.1625864E7 vendor_memoryPool_usage_max_bytes{name=\"PS Eden Space\"} 1.6777216E7 vendor_memoryPool_usage_max_bytes{name=\"PS Old Gen\"} 1.8408896E7 vendor_memoryPool_usage_max_bytes{name=\"PS Survivor Space\"} 5705344.0 call custom quarkus microprofile metrics exampleoc exec $(oc get pods -l app=backend | grep backend | head -n 1 | awk '{print $1}') \\ -- curl -s http://localhost:8080/q/metrics/application example result# TYPE application_org_acme_getting_started_BackendResource_timeBackend_seconds summary application_org_acme_getting_started_BackendResource_timeBackend_seconds_count 1.0 # TYPE application_org_acme_getting_started_BackendResource_timeBackend_seconds_sum gauge application_org_acme_getting_started_BackendResource_timeBackend_seconds_sum 2.503457774 application_org_acme_getting_started_BackendResource_timeBackend_seconds{quantile=\"0.5\"} 2.503457774 application_org_acme_getting_started_BackendResource_timeBackend_seconds{quantile=\"0.75\"} 2.503457774 application_org_acme_getting_started_BackendResource_timeBackend_seconds{quantile=\"0.95\"} 2.503457774 application_org_acme_getting_started_BackendResource_timeBackend_seconds{quantile=\"0.98\"} 2.503457774 application_org_acme_getting_started_BackendResource_timeBackend_seconds{quantile=\"0.99\"} 2.503457774 application_org_acme_getting_started_BackendResource_timeBackend_seconds{quantile=\"0.999\"} 2.503457774 Add Application Performance Metric to OpenShift create ServiceMonitor, go to Search in left menu, in search page, in resources drop down, type 'servicemonitor' for filter, click 'SM ServiceMonitor' Click Create ServiceMonitor button in Create ServiceMonitor Page, input below YAML for create ServiceMonitor to backend applicationapiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-app: backend-monitor name: backend-monitor spec: endpoints: - interval: 30s port: 8080-tcp path: /q/metrics scheme: http - interval: 30s port: 8080-tcp path: /q/metrics/application scheme: http selector: matchLabels: app: backend example: click create and review your ServiceMonitor, click YAML tab to view your yaml code. test call you backend application go to web terminal. test call your backend 2-3 timesBACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') curl $BACKEND_URL/backend click Monitor in left menu, select Metrics Tab in select query, change to custom query, type 'app' and wait auto suggesstion Remark: if you don't found metrics 'application*' in auto suggession, wait a few minute and retry again select 'application_org_acme_getting_started_BackendResource_countBackend_total', type enter. change your custom PromQL such as average tocal call backend service in 1 minute is type: 'rate(application_org_acme_getting_started_BackendResource_countBackend_total[1m])' enter Optional: test call backend 2-3 times and check metrics change in Monitoring Pages Add Alert to OpenShift Check PrometheusRule apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: backend-app-alert namespace: labels: openshift.io/prometheus-rule-evaluation-scope: leaf-prometheus spec: groups: - name: backend rules: - alert: HighLatency expr: application_org_acme_getting_started_BackendResource_timeBackend_max_seconds>1 labels: severity: 'critical' annotations: message: ' response time is sec' HightLatency Alert will fire when response time is greateer than 1 sec Create backend-app-alert click add icon (+) to open yaml editor input PrometheusRule yaml in editor and create (change namespace before run) Test HightLatency , run k6 as pod on OpenShift, change user1 to your username BACKEND_URL=https://$(oc get route backend -n -o jsonpath='{.spec.host}')/backend curl -o load-test-k6.js https://raw.githubusercontent.com/rhthsa/openshift-demo/main/manifests/load-test-k6.js oc run load-test -n -i \\ --image=loadimpact/k6 --rm=true --restart=Never \\ -- run - 25 threads Duration 2 minutes Ramp up 30 sec Ramp down 30 sec Check for alert in Developer Console by select Menu Observe then select Alerts Next Step Complex Cloud-Native Application with Live Flight Track Demo "},"liveflight.html":{"url":"liveflight.html","title":"Live Flight Track","keywords":"","body":"Live Flight Tracker Demo Live Flight Tracker Demo Architecture of the demo Step by Step to Deploy Live Flight Tracker on OpenShift Architecture of the demo map-service: web application with websocket, rest api for interact with browser for show and receive command for simulate live flight tracker postgresql: relational database for airport data data-simulator: backend service for simulation live flight (simulate radar data and transponder data to kafka) kafka broker: messaging system for integrate data between system data-aggregator: streams processing service for aggregate (merge) radar data and transponder data to flight data and put it back to kafka for display from map service Step by Step to Deploy Live Flight Tracker on OpenShift Create Project for Live Flight Tracker open openshift developer perspective, click \"Project\" from left menu, click \"create a Project\" in create project panel, set Name: \\-flight such as \"user1-flight\" click \"create\" Create Kafka on OpenShift click Topology in left menu click add icon (book with plus sign, right top, near Display options) type \"kafka\" in search box, select \"Kafka\" and click Create in Create Kafka, leave all default, click create wait until kafka complete deployment, you will see 3 deployment in kafka \"my-cluster\" (wait until all 3 deployment circle change to dark blue) Create Topic in kafka in Topology, click add icon (book with plus sign, right top, near Display options) type \"topic\", select Kafka Topic, click Create in create kafkatopic panel, change Name to \"flight-data\" leave other to default, click create repeat create kafkatopic with name \"radar-data\" and \"transponder-data\" check kafkatopic, in Topology, in my-cluster kafka, click on my-cluster-kafka statefulset (see example in below picture) in right panel popup, click in Resources tab, in Pods section, click view logs link in first pod (such as my-cluster-kafka-0) after page change to Pod information page, change tab from Logs to Terminal openshift will open terminal to pod, in terminal type below commond to check topic in kafka (flight-data, radar-data, transponder-data) ./bin/kafka-topics.sh --bootstrap-server=localhost:9092 --list sample output (if you create correct!)__consumer_offsets __strimzi_store_topic flight-data radar-data transponder-data Create Database Postgresql click +Add in left menu, in Developer Catalog, select Database type \"postgresql\" in search box, select PostgreSQL (Ephermeral) click Instantiate Template In Instantiate Template page, set value leave all default except, Database Service Name: \"postgresql\" Connection Username: \"flighttracker\" Conneciton Password: \"flighttracker\" Database Name: \"flighttracker\" click create wait until postgresql change to dark blue circle Deploy Data-Aggregator from Container Image click +Add left menu, select Container images in deploy image page, select Image name from external registry type \"quay.io/chatapazar1/data-aggregator\" runtime icon: change to quarkus Application: select Ceate Application Application Name: \"flight-data-aggregator\" Name: \"flight-data-aggregator\" resource: Deployment target port: 8080 uncheck create a route to the application click create wait until flight-data-aggregator circle change to dark blue color click flight-data-aggregator, select resources tab, click view logs in first port openshift console will display log of flight-data-aggregator, Verify that there are no errors Deploy Data-Simulator from Git Repository (S2I/OpenShift Build) click +Add in left menu in Import from Git page, set Git Repo URL to : https://github.com/chatapazar/openshift-workshop.git click show advanced git options set Context dir to : /data-simulator select Import Strategy to Builder Image select Java select Builder Image version to : openjdk-11-ubi8 in General, Application, select Create Application set Application name to : flight-data-simulator set name to : flight-data-simulator in resources, select deployment in advanced options, leave default target port uncheck create a route to the application and click Create in Topology view, click circle \"(D) flight-data-simulator\" for show deployment information popup in right side of developer console in flight-data-simulator information page, click Resources tab, scroll down to Builds section, wait until Build #1 was complete or click View logs for view s2i build your code to container image after build complete, flight-data-simulater circle will change to dark blue color, click view logs in Pods section to view application log, Verify that there are no errors Deploy Map-Service from Git Repository (S2I/OpenShift Build) click +Add in left menu, select Import from Git in Import from Git page, set Git Repo URL to : https://github.com/chatapazar/openshift-workshop click show advanced Git options set Context dir to : /map-service Import Strategy select Builder Image, select Java, set Builder Image version to : openjdk-11-ubi8 Application set to Create Application Application name: flight-map-service Name: flight-map-service in resources section, select deployment, leave default target port 8080 uncheck Create a route to the Applicaiton, click Create in Topology view, click circle \"(D) flight-map-service\" for show deployment information popup in right side of developer console in flight-map-service information page, click Resources tab, scroll down to Builds section, wait until Build #1 was complete or click View logs for view s2i build your code to container image after build complete, flight-data-simulater circle will change to dark blue color, click view logs in Pods section to view application log, Verify that there are no errors Create Route for flight-map-service click Project in left menu, select Project: \\-flight in Overview tab, scroll down to Inventory section, select Route in Route page, click Create Route in create route page, leave all default except: name: flight-map-service Service: select flight-map-service target port: 8080-->8080(TCP) uncheck secure route, click create back to topology view, in flight-map-service, You will see a route icon in the top right corner of flight-map-service circle (or click flight-map-service circle and see details in flight-map-service deployment information page, resources tab) (Optional) Change Icon and Add Connection Info in Topology (add more information in Topology) back to Topology view (click Topology in left menu) click flight-data-simulator circle, in (D) flight-data-simulator deployment information, select details tab scroll down to Labels section click edit link in edit labels popup, remove label \"app.openshift.io/runtime=java\" (click x icon) type \"app.openshift.io/runtime=quarkus\" to add new icon to this deployment, click save verify flight-data-simulator change icon from duke to quarkus Repeat with the other components as follows: flight-map-service, remove label \"app.openshift.io/runtime=java\", add label \"app.openshift.io/runtime=quarkus\" postgresql, add label \"app.openshift.io/runtime=postgresql\" add connection to topology (For cosmetic purposes only, not a real conneciton between components.), click web terminal icon at top right conner (\">_\" icon) wait until command line terminal show in buttom of developer console set current project to your project ('\\-flight') such as user1-flightoc project -flight sample outputNow using project \"user1-flight\" on server \"https://172.30.0.1:443\". set connection with oc annotate command, type below command in web terminaloc annotate deployment flight-map-service 'app.openshift.io/connects-to=[{\"apiVersion\":\"apps.openshift.io/v1\",\"kind\":\"DeploymentConfig\",\"name\":\"postgresql\"},{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"flight-data-simulator\"},{\"apiVersion\":\"kafka.strimzi.io/v1beta2\",\"kind\":\"Kafka\",\"name\":\"my-cluster\"}]' oc annotate deployment flight-data-simulator 'app.openshift.io/connects-to=[{\"apiVersion\":\"kafka.strimzi.io/v1beta2\",\"kind\":\"Kafka\",\"name\":\"my-cluster\"}]' oc annotate deployment flight-data-aggregator 'app.openshift.io/connects-to=[{\"apiVersion\":\"kafka.strimzi.io/v1beta2\",\"kind\":\"Kafka\",\"name\":\"my-cluster\"}]' check result in topology view Test Live Flight Track Demo in topology view, click route icon of flight-map-service for open web application browser will show Live Flight Tracer demo click checkbox show slifht creation form and show websocket log click start this flight button or start 20 random flights button and see simulation in your browser "},"evconfigsecret.html":{"url":"evconfigsecret.html","title":"App Configuration","keywords":"","body":"Configuration Management with Environment Variable, Configmap & Secret Configuration Management with Environment Variable, Configmap & Secret Prerequisite Environment Variable Configmag Secret Next Step Prerequisite Complete Deploy application to openshift with s2i Go to your project (same as your username) Open Web Terminal by click '>_' on top of OpenShift Web Console use web terminal to run command line Environment Variable You can set environment variables for containers running in a pod. Additionally, Kubernetes automatically exposes certain runtime information via environment variables. Review Code, Check Environment Variable https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/src/main/resources/application.properties In this properties, app.backend is url of backend service, we use this property for Rest Client app.backend=https://httpbin.org/status/200 test call backend service in web terminal console curl -v https://httpbin.org/status/200 curl -v https://httpbin.org/status/400 Check Current Environment, Test with below command BACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') curl $BACKEND_URL/backend sample output, see Response:200 Backend version:v1, Response:200, Host:backend-56957dcc4f-t9bpf, Status:200, Message: Hello, World Change Environment Variable, From Topology, click Duke icon (backend deployment), backend link in side panel (click link at 'D backend') select Environment tab, in single values(env), input name: 'app.backend' and value: 'https://httpbin.org/status/400', clicke save wait until openshift redeploy backend success back to web terminal, test call backend againBACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') curl $BACKEND_URL/backend check output change, Response:400 Backend version:v1, Response:400, Host:backend-6cf95f994d-cgxsb, Status:400, Message: Hello, World Configmag Many applications require configuration using some combination of configuration files, command line arguments, and environment variables. These configuration artifacts should be decoupled from image content in order to keep containerized applications portable. The ConfigMap object provides mechanisms to inject containers with configuration data while keeping containers agnostic of OpenShift Container Platform. A ConfigMap can be used to store fine-grained information like individual properties or coarse-grained information like entire configuration files or JSON blobs. create example ConfigMap, click ConfigMaps at left menu, click Create ConfigMap Create ConfigMap with YAML, copy below yaml to editor, click create Remark: change namespace to your usernameapiVersion: v1 kind: ConfigMap metadata: name: example namespace: data: app.backend.200: https://httpbin.org/status/200 app.backend.400: https://httpbin.org/status/400 Review configmap 'example' value in data section back to Topology, click Duke icon (backend deployment), backend link in side panel (click link at 'D backend'), click Environment Tab remove old app.backend environment variable click 'Add from ConfigMap or Secret', set name: 'app.backend', value: 'CM example' and key: 'app.backend.200' click save, and wait until backend redeploy complete back to web terminal, test call backend againBACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') curl $BACKEND_URL/backend check output change, Response:200 Backend version:v1, Response:200, Host:backend-95647fbb8-2xql7, Status:200, Message: Hello, World Secret The Secret object type provides a mechanism to hold sensitive information such as passwords, OpenShift Container Platform client configuration files, dockercfg files, private source repository credentials, and so on. Secrets decouple sensitive content from the pods. You can mount secrets into containers using a volume plug-in or the system can use secrets to perform actions on behalf of a pod. at web terminal, test curret value of secret message : 'Not Found Secret'curl $BACKEND_URL/backend/showsecret example resultBackend version:v1, Response:200, Host:backend-95647fbb8-62584, Status:200, Message: Not Found Secret click Secrets from left menu select create with key/value secret in create key/value secret page, set secret name: example, key: 'app.secretMessage', value: 'Hi! World of Quarkus!', click Create check in secret 'example', in data section, show 'app.secretMessage' and value with encryption (you can click copy and paste in your editor to view message) back to Topology, click Duke icon (backend deployment), backend link in side panel (click link at 'D backend'), click Environment Tab click 'Add from ConfigMap or Secret', set name: 'app.secretMessage', value: 'S example' and key: 'app.secretMessage' click save, wait until backend redeploy complete Re-Test showsecret api againcurl $BACKEND_URL/backend/showsecret example resultBackend version:v1, Response:200, Host:backend-5bc95899d-tdmv5, Status:200, Message: Hi! World of Quarkus! Next Step Application Health check "},"apphealth.html":{"url":"apphealth.html","title":"App Health check","keywords":"","body":"Application Health check Application Health check Prerequisite Application Probe Set Application Health Check Test Liveness Probe Test Readiness Probe Remove Application Health Next Step Prerequisite Complete Deploy application to openshift with s2i Go to your project (same as your username) Open Web Terminal by click '>_' on top of OpenShift Web Console use web terminal to run command line Application Probe A probe is a Kubernetes action that periodically performs diagnostics on a running container. Currently, two types of probes exist, each serving a different purpose. Readiness Probe A Readiness check determines if the container in which it is scheduled is ready to service requests. If the readiness probe fails a container, the endpoints controller ensures the container has its IP address removed from the endpoints of all services. A readiness probe can be used to signal to the endpoints controller that even though a container is running, it should not receive any traffic from a proxy. For example, a Readiness check can control which Pods are used. When a Pod is not ready, it is removed. Liveness Probe A Liveness checks determines if the container in which it is scheduled is still running. If the liveness probe fails due to a condition such as a deadlock, the kubelet kills the container The container then responds based on its restart policy. Review Application Liveness Probe review code URL: https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/src/main/java/org/acme/microprofile/health/SimpleHealthCheck.java go to web terminal, connect to backend podPOD=$(oc get pods --no-headers -l app=backend | grep backend |head -n 1| awk '{print $1}') oc rsh $POD after connect to pod, shell will change from 'bash-4.4' to 'sh-4.4'bash-4.4 ~ $ oc rsh $POD sh-4.4$ test call liveness probe of application curl -v http://localhost:8080/q/health/live example output test stop liveness (simulate pod not working) and test call liveness probe of application curl http://localhost:8080/backend/stop curl -v http://localhost:8080/q/health/live example output start liveness again and test call liveness probe of application curl http://localhost:8080/backend/start curl -v http://localhost:8080/q/health/live example output exit from podexit Review Application Readiness Probe review code URL: https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/src/main/java/org/acme/microprofile/health/DatabaseConnectionHealthCheck.java click topology in left menu, click Duke icon (backend deployment), click pod name link (such as 'P backend-xxxx-xxxxx' link) in Pod details, click Terminal Tab, web console will show terminal to backend pod. test call readyness probe of application curl -v http://localhost:8080/q/health/ready example output test down readyness prove of application and re-test again curl http://localhost:8080/databasestatus/down curl -v http://localhost:8080/q/health/ready example output up readyness probe of application, and re-test again curl http://localhost:8080/databasestatus/up curl -v http://localhost:8080/q/health/ready example output back to Topology Page Set Application Health Check go to web terminal pause auto re-deploy from deployment update configuration triggeroc rollout pause deployment/backend set readiness probe and liveness probe, resume deployment update configuration triggeroc set probe deployment/backend --readiness --get-url=http://:8080/q/health/ready --initial-delay-seconds=20 --failure-threshold=1 --period-seconds=3 --timeout-seconds=5 oc set probe deployment/backend --liveness --get-url=http://:8080/q/health/live --initial-delay-seconds=20 --failure-threshold=1 --period-seconds=10 --timeout-seconds=5 oc rollout resume deployment/backend wait until backend re-deploy complete view current health check, click topology in left menu, click Duke icon (backend deployment), at actions menu, select edit Health Checks, view current Health checks Test Liveness Probe go to web terminal check current podoc get pods -l app=backend example result, check restarts is 0NAME READY STATUS RESTARTS AGE backend-58769d5765-dw6fc 1/1 Running 0 15h call stop api for simulate pod stop, go to web terminalPOD=$(oc get pods --no-headers -l app=backend | grep backend |head -n 1| awk '{print $1}') oc exec $POD -- curl -s http://localhost:8080/backend/stop check liveness probe error oc describe pod $POD example result, see Liveness probe failed, after that openshift will auto-restart pod... Warning Unhealthy 7s (x2 over 7m27s) kubelet Liveness probe failed: HTTP probe failed with statuscode: 503 Normal Killing 7s (x2 over 7m27s) kubelet Container backend failed liveness probe, will be restarted Normal Pulling 7s (x4 over 19m) kubelet Pulling image \"image-registry.openshift-image-registry.svc:5000/user1/backend@sha256:c444188385159d4b1c7fe493334518a53d26c179534572f48c96a719d747064a\" Normal Started 6s (x3 over 19m) kubelet Started container backend Normal Created 6s (x3 over 19m) kubelet Created container backend check result, openshift will auto-restart pod after liveness probe erroroc get pods -l app=backend example result, check restarts is 1NAME READY STATUS RESTARTS AGE backend-58769d5765-dw6fc 1/1 Running 1 15h Test Readiness Probe go to web terminal scale pod to 2 oc scale deployment/backend --replicas=2 check, have 2 pod of backendoc get pod -l app=backend example result, check ready is 1/1 (wait until both pod running and ready is 1/1)NAME READY STATUS RESTARTS AGE backend-87784db56-2642v 1/1 Running 0 2m38s backend-87784db56-swg4m 1/1 Running 0 6m19s check service call to both podsoc describe service backend example result, in endpoints section have 2 ipaddress from both pods.... IP: 172.30.76.111 IPs: 172.30.76.111 Port: 8080-tcp 8080/TCP TargetPort: 8080/TCP Endpoints: 10.131.0.43:8080,10.131.0.44:8080 Port: 8443-tcp 8443/TCP TargetPort: 8443/TCP Endpoints: 10.131.0.43:8443,10.131.0.44:8443 Port: 8778-tcp 8778/TCP TargetPort: 8778/TCP Endpoints: 10.131.0.43:8778,10.131.0.44:8778 Session Affinity: None Events: ... test call backend api from route, call 2-4 times to check response from both podsBACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') curl $BACKEND_URL/backend example responseBackend version:v1, Response:200, Host:backend-58769d5765-dw6fc, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-87784db56-2642v, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-58769d5765-dw6fc, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-87784db56-2642v, Status:200, Message: Hello, World set readiness prove of backend to downPOD=$(oc get pods --no-headers -l app=backend | grep backend |head -n 1| awk '{print $1}') oc exec $POD -- curl -s http://localhost:8080/databasestatus/down check readiness probe erroroc describe pod $POD example result, see Readiness probe failed bash Normal Killing 18m (x2 over 25m) kubelet Container backend failed liveness probe, will be restarted Normal Created 18m (x3 over 37m) kubelet Created container backend Normal Started 18m (x3 over 37m) kubelet Started container backend Warning Unhealthy 3s (x7 over 21s) kubelet Readiness probe failed: HTTP probe failed with statuscode: 503bash check pod not readyoc get pods -l app=backend example result, check ready change to 0/1 in one podNAME READY STATUS RESTARTS AGE backend-87784db56-2642v 0/1 Running 0 9m48s backend-87784db56-swg4m 1/1 Running 0 13m check service againoc describe service backend example result, endpoints has only one ipaddress.... IP: 172.30.76.111 IPs: 172.30.76.111 Port: 8080-tcp 8080/TCP TargetPort: 8080/TCP Endpoints: 10.131.0.43:8080 Port: 8443-tcp 8443/TCP TargetPort: 8443/TCP Endpoints: 10.131.0.43:8443 Port: 8778-tcp 8778/TCP TargetPort: 8778/TCP Endpoints: 10.131.0.43:8778 Session Affinity: None Events: test call backend againBACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') curl $BACKEND_URL/backend example result, call 2-4 tiems, have only response from 1 pod.Backend version:v1, Response:200, Host:backend-87784db56-swg4m, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-87784db56-swg4m, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-87784db56-swg4m, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-87784db56-swg4m, Status:200, Message: Hello, World set readiness probe up againPOD=$(oc get pods --no-headers -l app=backend | grep backend |head -n 1| awk '{print $1}') oc exec $POD -- curl -s http://localhost:8080/databasestatus/up check readyoc get pods -l app=backend example result, ready change back to 1/1NAME READY STATUS RESTARTS AGE backend-87784db56-2642v 1/1 Running 0 15m backend-87784db56-swg4m 1/1 Running 0 18m re-test call backend again, test call 2-4 timesBACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') curl $BACKEND_URL/backend example output, response from 2 pods again.Backend version:v1, Response:200, Host:backend-58769d5765-dw6fc, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-87784db56-2642v, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-58769d5765-dw6fc, Status:200, Message: Hello, World Backend version:v1, Response:200, Host:backend-87784db56-2642v, Status:200, Message: Hello, World set repica back to 1 oc scale deployment/backend --replicas=1 Remove Application Health view current health check, click topology in left menu, click Duke icon (backend deployment), at actions menu, select edit Health Checks, view current Health checks remove readiness and liveness health check (click minus icon), click save and wait until backend redeploy complete (change circle aroubd duke icon to dark blue) Next Step Viewing and working with logs generated by your application "},"logging.html":{"url":"logging.html","title":"Logging","keywords":"","body":"Viewing and working with logs generated by your application Viewing and working with logs generated by your application Prerequisite Review Application Code Logging Monitor Container Log with Developer Console Monitor Container Log with EFK Stack in Openshift Next Step Prerequisite Complete Deploy application to openshift with s2i Go to your project (same as your username) Open Web Terminal by click '>_' on top of OpenShift Web Console use web terminal to run command line Review Application Code Logging Example code with logging (Quarkus Framework) Code URL: https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/src/main/java/org/acme/getting/started/BackendResource.java Example Code Loggingimport org.jboss.logging.Logger; ... private static final Logger logger = Logger.getLogger(BackendResource.class); ... URL url; try { logger.info(\"Request to: \" + backend); ... example log property (Quarkus Framework) Properties URL: https://raw.githubusercontent.com/chatapazar/openshift-workshop/main/src/main/resources/application.properties Example properties:#Logging quarkus.log.level=INFO # quarkus.log.category.\"com.example.quarkus\".level=INFO # quarkus.log.category.\"com.example.quarkus.health\".level=DEBUG quarkus.log.console.enable=true quarkus.log.console.format=%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n quarkus.log.console.color=false %dev.quarkus.log.console.color=true Monitor Container Log with Developer Console go to web terminal test call backend serviceBACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') curl $BACKEND_URL/backend view log in pod, go to Topology, click Duke icon (backend), in backend deployment select Resources Tab, click 'View logs' of Pod in pod details, select Logs tab to view log of container 'backend' re call backend service and check log in pod append (retry call 2-3 times for view logs append) BACKEND_URL=https://$(oc get route backend -o jsonpath='{.spec.host}') curl $BACKEND_URL/backend check log append at log terminal click raw icon to view log in another browser tab click download to download currnet log Monitor Container Log with EFK Stack in Openshift From Logs tab in Pod details, click show in Kibana icon to open kibana in new tab Optional: for firstime, openshift will request you re-login and allow authorized to kibana (with oauth), click allow selected permissions Openshift will Open Kibana from EFK Stack, default page is Discover and auto add filter with pod name, namespace/project and container name Developer can custom filter to view log. For monitor more than 1 pod, developer can access kibana from logging menu on top of Developer Console (near web terminal menu icon) with this link, developer can custom filter to view log. Next Step Basic Container Storage "},"storage.html":{"url":"storage.html","title":"Storage","keywords":"","body":"Container Storage Container Storage Prerequisite Test Create File in Container Create Persistence Data for Container Next Step Prerequisite Complete Deploy application to openshift with s2i Go to your project (same as your username) Open Web Terminal by click '>_' on top of OpenShift Web Console use web terminal to run command line Test Create File in Container From Topology view, click Duke icon (backend deployment), in backend deployment view, select Resources Tab Click link of pod name in Pods section in Pod Details, select Terminal Tab to enable Terminal to container In terminal console, test create file in containercd /opt echo 'This is a test' > data.txt cat /opt/data.txt example scale down backend application to 0, back to topology, click Duke icon, in details tab of backend Deployment, click scale down to 0 after scale to 0 complete, scale up backend application to 1 again Recheck data in container, click at resource tab in backend deployment, select pod 'backend-xxx-xxx' link in Pods section in pod details, select terminal tabs to activate terminal console to pod re test with below command, data.txt not found in current container becouse we don't have persistent storage for it.cd /opt ls cat /opt/data.txt example output Create Persistence Data for Container back to topology, click Duke icon to open backend deployment panel, select action menu dropdownlist, select add storage in add storage, select Persistent Volume Claim: create new claim storage class: gp2 Persistent Volume Claim Name: 'userx-cliam' (change userx to your user name) access mode: Single User(RWO) size: 1 GiB Volume Mode: Filesystem Mount Path: /data example result click save and wait until backend redeploy complete in backend deployment, go to pods tabs, select pod name link 'backend-xxx-xxx' go to terminal tab again to open terminal to container, create file again for test persistentcd /data echo 'This is a test' > data.txt cat /data/data.txt example output bact to topology, click duke icon, in Details Tab, scale down backend deployment to 0 and scale up to 1 again (wait until scale complete before go to next step) go to pod terminal again, go to Resources Tab, click pod link 'backend-xxx-xxx' in pod section go to terminal tab to open terminal and retest with below commandcd /data ls cat data.txt example output, you will found data.txt Next Step Basic Serverless, auto scale up & scale down by request "},"serverless.html":{"url":"serverless.html","title":"Serverless","keywords":"","body":"Basic Serverless, auto scale up & scale down by request Basic Serverless, auto scale up & scale down by request Prerequisite Change your application to Serverless Next Step Prerequisite Complete Deploy application to openshift with s2i Go to your project (same as your username) Open Web Terminal by click '>_' on top of OpenShift Web Console use web terminal to run command line Change your application to Serverless From Topology view, open web terminal, delete route, service and deployment of backend with below command (change project name to your project name such as 'userx') oc project user1 oc delete route/backend oc delete service/backend oc delete deployment/backend redeploy application again with openshift serverless (serving) feature, click Add+ in left menu, select deploy from container images in image section, select Image stream tag from internal registry, select project 'userx' (your username), image stream 'backend' and Tag 'latest' in general section, set Application name: 'backend', Name: 'backend' in resources seciton, select 'Serverless Deployment' for advanced options, leave all default, click create wait until deploy complete, click KSVC 'backend' test backend application click route icon to open application in new tab wait until application auto scale down (1 minute) in topology view, no pod start and deployment show 0 pod test call application again by click route, serverless will automatic start pod. Next Step Complex Cloud-Native Application with Live Flight Track Demo "}}